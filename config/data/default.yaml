# Data loader
_recursive_: False
_target_: forward.model.data_loader.CRTMDataloader

# Data loader parameters
dir: ${paths.data_dir}  # Directory where the data is stored
batch_size: 32  # Batch size
sets:
  train:
    split: 256608
    path: ${data.dir}/Train2
    prof:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/prof.npy
      normalization:
        _target_: forward.data.transformations.NormalizeProfiles
        _partial_: false
        profmax:
          _target_: numpy.loadtxt
          _args_:
            - forward/data/normalization/prof_div.txt
        profmin:
          _target_: numpy.loadtxt
          _args_:
            - forward/data/normalization/prof_m.txt
    surf:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/surf.npy
      normalization:
        _target_: forward.data.transformations.NormalizeSurface
        _partial_: false
        surfmax:
          _target_: numpy.loadtxt
          _args_:
            - forward/data/normalization/surfmax.txt
        surfmin:
          _target_: numpy.loadtxt
          _args_:
            - forward/data/normalization/surfmin.txt
    meta:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/meta.npy
      normalization:
        _target_: forward.data.transformations.NormalizeMeta
        _partial_: false
        settings:
          meta_cos_vars: [ 6 ]
          meta_scale_factor: 180
          meta_scale_offset: -180
          meta_scale_vars: [ ]
          meta_sin_vars: [ 0, 1, 2, 3, 4, 5 ]
    hofx:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/hofx.npy
      normalization:
        _target_: forward.data.transformations.identity
        _partial_: true
    lat:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/lat.npy
    lon:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/lon.npy
    scans:
      load:
        _target_: numpy.loadtxt
        _args_:
          - ${data.sets.train.path}/scans.txt
    pressure:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.train.path}/pressure.npy
  valid:
    split: 85532
    path: ${data.dir}/Valid2
    prof:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/prof.npy
      normalization: ${data.sets.train.prof.normalization}
    surf:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/surf.npy
      normalization: ${data.sets.train.surf.normalization}
    meta:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/meta.npy
      normalization: ${data.sets.train.meta.normalization}
    hofx:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/hofx.npy
      normalization: ${data.sets.train.hofx.normalization}
    lat:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/lat.npy
    lon:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/lon.npy
    scans:
      load:
        _target_: numpy.loadtxt
        _args_:
          - ${data.sets.valid.path}/scans.txt
    pressure:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.valid.path}/pressure.npy
  test:
    path: ${data.dir}/Test2
    prof:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/prof.npy
      normalization: ${data.sets.train.prof.normalization}
    surf:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/surf.npy
      normalization: ${data.sets.train.surf.normalization}
    meta:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/meta.npy
      normalization: ${data.sets.train.meta.normalization}
    hofx:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/hofx.npy
      normalization: ${data.sets.train.hofx.normalization}
    lat:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/lat.npy
    lon:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/lon.npy
    scans:
      load:
        _target_: numpy.loadtxt
        _args_:
          - ${data.sets.test.path}/scans.txt
    pressure:
      load:
        _target_: numpy.load
        _args_:
          - ${data.sets.test.path}/pressure.npy
    results:
      hofx:
        save:
          _target_: numpy.save
          _partial_: true
          _args_:
            - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
        load:
          _target_: numpy.load
          _args_:
            - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
        normalization: ${data.sets.train.hofx.normalization}
  pred:
    path: ${hydra:runtime.output_dir}
    prof: ${data.sets.test.prof}
    surf: ${data.sets.test.surf}
    meta: ${data.sets.test.meta}
    hofx:
      save:
        _target_: numpy.save
        _partial_: true
        _args_:
          - ${hydra:runtime.output_dir}/hofx_lightning.npy
      load:
        _target_: numpy.load
        _args_:
          - ${hydra:runtime.output_dir}/hofx_lightning.npy
    lat: ${data.sets.test.lat}
    lon: ${data.sets.test.lon}
    scans: ${data.sets.test.scans}
    pressure: ${data.sets.test.pressure}
  # Statistics
  preparation:
    statistics:
      # Input
      input:
        # Path
        data: ${data.sets.train}
      # Output
      output:
        # Values
        data:
          # Path
          path: ${data.dir}/normalization/stats.pkl
          type: "file"
