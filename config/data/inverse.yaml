# Note: This configuration file is for the ***inverse*** model.


# Location of the data directory  # TODO: Maybe remove this line
dir: ${paths.data_dir}


# Configuration for the variables used in the data loader.
vars:
  prof:
    type: ['Air temperature', 'Water vapor mixing ratio', 'Cloud ice mass', 'Cloud water mass', 'Snow mass',
           'Ice particle effective radius', 'Water particle effective radius',
           'Snow particle effective radius', 'Ozone mixing ratio']
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/prof.npy
    normalization: ${model.transform}
  surf:
    type: ['Ice area fraction', 'Land area fraction', 'Land type', 'Leaf area index', 'Soil temperature',
           'Snow area fraction', 'Snow thickness', 'Soil temperature',
           'Surface temperature (ice, land, sea, snow)', 'Wind direction', 'Wind speed',
           'Vegetation area fraction', 'Water area fraction', 'Volumetric water ratio (soil)']
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/surf.npy
    normalization:
      _target_: forward.data.transformations.NormalizeSurface
      _partial_: false
      surfmax:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/surfmax.txt
      surfmin:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/surfmin.txt
  meta:
    type: ['Sensor scan angle', 'Sensor zenith angle', 'Sensor view angle', 'Sensor azimuth angle',
            'Sensor elevation angle', 'Solar azimuth angle', 'Solar zenith angle']
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/meta.npy
    normalization:
      _target_: forward.data.transformations.NormalizeMeta
      _partial_: false
      settings:
        meta_cos_vars: [ 6 ]
        meta_scale_factor: 180
        meta_scale_offset: -180
        meta_scale_vars: [ ]
        meta_sin_vars: [ 0, 1, 2, 3, 4, 5 ]
  hofx:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/hofx.npy
    normalization:
      _target_: forward.data.transformations.identity
      _partial_: true
  lat:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/lat.npy
  lon:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/lon.npy
  scans:
    load:
      _target_: numpy.loadtxt
      _args_:
        - ${paths.data_dir}/Test2/scans.txt
  pressure:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/pressure.npy
  background:
    load:
      _target_: inverse.data.covariance.spatiotemporal_mean
      _args_:
        - ${data.vars.prof.load}
    normalization: ${data.vars.prof.normalization}
  background_err:
    load:
      _target_: inverse.data.covariance.compute_err
      _args_:
        - ${data.vars.prof.load}
        - ${data.vars.background.load}
      normalization:
        _target_: forward.data.transformations.max
        _partial_: true
  pressure_filter:
    load:
      _target_: inverse.data.filters.pressure_filter
      _args_:
        - ${data.vars.prof.load}
  cloud_filter:
    load:
      _target_: inverse.data.filters.cloud_filter
      _args_:
        - ${data.vars.prof.load}


# Preparation steps to be executed before training or evaluation.
preparation:
  statistics:
    input: ${data.vars}
    output:
      path: ${paths.data_dir}/normalization/stats.pkl
      type: file
  covariance:
    model:
      input:
        prof: ${data.vars.prof}
        background: ${data.vars.background}
        cloud_filter: ${data.loader.sets.split.cloud_filter}
        pressure_filter: ${data.vars.pressure_filter}
      output:
        path: ${paths.data_dir}/normalization/model_covariance_cloud_filter_pressure_filter.npy
        type: file
    observation:
      input:
          hofx: ${data.vars.hofx}
          cloud_filter: ${data.loader.sets.split.cloud_filter}
      output:
        path: ${paths.data_dir}/normalization/observation_covariance_cloud_filter.npy
        type: file


# Configuration for the data loader that handles the dataset splits.
loader:
  _target_: forward.model.data_loader.VarDataloader
  batch_size: 32  # Batch size for training
  num_workers: 1  # Number of workers for data loading
  sets:
    split:
      train: 0:256608
      valid: 256608:341240
      test: 341240:426872
      cloud_filter: ${data.vars.cloud_filter}
    coordinates:
      lat: ${data.vars.lat}
      lon: ${data.vars.lon}
      scans: ${data.vars.scans}
      pressure: ${data.vars.pressure}
    targets:
      prof: ${data.vars.prof}
      surf: ${data.vars.surf}
      meta: ${data.vars.meta}
      hofx: ${data.vars.hofx}
      background: ${data.vars.background}
    # outputs:


# Configuration for the data loader that handles the dataset splits.
# loader:
#   _target_: forward.model.data_loader.VarDataloader
#   batch_size: 32  # Batch size for training
#   num_workers: 1  # Number of workers for data loading
#   sets:
#     train:
#       split:
#         range: 0:256608
#         cloud_filter: ${data.vars.cloud_filter}
#       prof_type: ${data.vars.prof.type}  # Type of profile to use
#       coordinates:
#         lat: ${data.vars.lat}
#         lon: ${data.vars.lon}
#         scans: ${data.vars.scans}
#         pressure: ${data.vars.pressure}
#       targets:
#         prof: ${data.vars.prof}
#         surf: ${data.vars.surf}
#         meta: ${data.vars.meta}
#         hofx: ${data.vars.hofx}
#         background: ${data.vars.background}
#     valid:
#       split:
#         range: 256608:341240
#         cloud_filter: ${data.loader.sets.train.split.cloud_filter}
#       prof_type: ${data.loader.train.prof_type}
#       coordinates: ${data.loader.train.coordinates}
#       targets: ${data.loader.train.targets}
#     test:
#       split:
#         range: 341240:426872
#         cloud_filter: ${data.loader.sets.train.split.cloud_filter}
#       prof_type: ${data.loader.train.prof_type}
#       coordinates: ${data.loader.train.coordinates}
#       targets: ${data.loader.train.targets}
#       results:
#         hofx:
#           save:
#             _target_: numpy.save
#             _partial_: true
#             _args_:
#               - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
#           load:
#             _target_: numpy.load
#             _args_:
#               - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
#           normalization: ${vars.hofx.normalization}
#         prof:
#           save:
#             _target_: numpy.save
#             _partial_: true
#             _args_:
#               - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
#           load:
#             _target_: numpy.load
#             _args_:
#               - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
#     pred:
#       prof_type: ${data.vars.prof.type}  # Type of profile to use
#       coordinates: ${data.loader.train.coordinates}