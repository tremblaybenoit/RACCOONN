# Note: This configuration file is for the ***inverse*** model.


# Location of the data directory  # TODO: Maybe remove this line
dir: ${paths.data_dir}


# Configuration for the variables used in the data loader.
vars:
  prof:
    type: ['Air temperature', 'Water vapor mixing ratio', 'Cloud ice mass', 'Cloud water mass', 'Snow mass',
           'Ice particle effective radius', 'Water particle effective radius',
           'Snow particle effective radius', 'Ozone mixing ratio']
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/prof.npy
    normalization:
      _target_: forward.data.transformations.NormalizeProfiles
      _partial_: false
      profmax:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/prof_div.txt
      profmin:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/prof_m.txt
  surf:
    type: ['Ice area fraction', 'Land area fraction', 'Land type', 'Leaf area index', 'Soil temperature',
           'Snow area fraction', 'Snow thickness', 'Soil temperature',
           'Surface temperature (ice, land, sea, snow)', 'Wind direction', 'Wind speed',
           'Vegetation area fraction', 'Water area fraction', 'Volumetric water ratio (soil)']
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/surf.npy
    normalization:
      _target_: forward.data.transformations.NormalizeSurface
      _partial_: false
      surfmax:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/surfmax.txt
      surfmin:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/surfmin.txt
  meta:
    type: ['Sensor scan angle', 'Sensor zenith angle', 'Sensor view angle', 'Sensor azimuth angle',
            'Sensor elevation angle', 'Solar azimuth angle', 'Solar zenith angle']
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/meta.npy
    normalization:
      _target_: forward.data.transformations.NormalizeMeta
      _partial_: false
      settings:
        meta_cos_vars: [ 6 ]
        meta_scale_factor: 180
        meta_scale_offset: -180
        meta_scale_vars: [ ]
        meta_sin_vars: [ 0, 1, 2, 3, 4, 5 ]
  hofx:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/hofx.npy
    normalization:
      _target_: forward.data.transformations.identity
      _partial_: true
  lat:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/lat.npy
  lon:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/lon.npy
  scans:
    load:
      _target_: numpy.loadtxt
      _args_:
        - ${paths.data_dir}/Test2/scans.txt
  pressure:
    load:
      _target_: numpy.load
      _args_:
        - ${paths.data_dir}/Test2/pressure.npy


# Preparation steps to be executed before training or evaluation.
preparation:
  statistics:
    input: ${data.vars}
    output:
      path: ${paths.data_dir}/normalization/stats.pkl
      type: file


# Configuration for the data loader that handles the dataset splits.
loader:
  _target_: forward.model.data_loader.VarDataloader
  batch_size: 32  # Batch size for training
  num_workers: 1  # Number of workers for data loading
  cloud_filter: true  # Use cloud filter to remove clear sky data
  prof_filter: true  # Use profile filter to remove profiles with zero variance
  prof_type: ${data.vars.prof.type}  # Type of profile to use
  coordinates: ['lat', 'lon', 'scans', 'pressure']
  targets: ['prof', 'surf', 'meta', 'hofx']
  sets:
    split:
      train: 0:256608
      valid: 256608:341240
      test: 341240:426872
    prof_type: ${data.vars.prof.type}  # Type of profile to use
    coordinates:
      lat: ${data.vars.lat}
      lon: ${data.vars.lon}
      scans: ${data.vars.scans}
      pressure: ${data.vars.pressure}
    targets:
      prof: ${data.vars.prof}
      surf: ${data.vars.surf}
      meta: ${data.vars.meta}
      hofx: ${data.vars.hofx}


# Configuration for the data loader that handles the dataset splits.
loader:
  _target_: forward.model.data_loader.VarDataloader
  batch_size: 32  # Batch size for training
  num_workers: 1  # Number of workers for data loading
  cloud_filter: true  # Use cloud filter to remove clear sky data
  prof_filter: true  # Use profile filter to remove profiles with zero variance
  prof_type: ${data.vars.prof.type}  # Type of profile to use
  coordinates: ['lat', 'lon', 'scans', 'pressure']
  targets: ['prof', 'surf', 'meta', 'hofx']
  sets:
    train:
      split: 0:256608
      vars:
        lat: ${data.vars.lat}
        lon: ${data.vars.lon}
        scans: ${data.vars.scans}
        pressure: ${data.vars.pressure}
        prof: ${data.vars.prof}
        surf: ${data.vars.surf}
        meta: ${data.vars.meta}
        hofx: ${data.vars.hofx}
    valid:
      split: 256608:341240
      vars: ${data.loader.train.vars}
    test:
      split: 341240:426872
      vars: ${data.loader.train.vars}
      results:
        hofx:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          normalization: ${vars.hofx.normalization}
        prof:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy


# Configuration for the data loader that handles the dataset splits.
loader:
  _target_: forward.model.data_loader.VarDataloader
  dir: ${data.dir}  # Directory where the data is stored
  batch_size: 32  # Batch size for training
  num_workers: 1  # Number of workers for data loading
  cloud_filter: true  # Use cloud filter to remove clear sky data
  prof_filter: true  # Use profile filter to remove profiles with zero variance
  sets:
    train:
      split: 0:256608
      prof_type: ${data.vars.prof.type}  # Type of profile to use
      coordinates:
        lat: ${data.vars.lat}
        lon: ${data.vars.lon}
        scans: ${data.vars.scans}
        pressure: ${data.vars.pressure}
        # TODO: Add pressure_filter?
      targets:
        prof: ${data.vars.prof}
        surf: ${data.vars.surf}
        meta: ${data.vars.meta}
        hofx: ${data.vars.hofx}
        # TODO: Add to vars
        background: ${data.vars.background}
    valid:
      split: 256608:341240
      prof_type: ${data.loader.train.prof_type}
      coordinates: ${data.loader.train.coordinates}
      targets: ${data.loader.train.targets}
    test:
      split: 341240:426872
      prof_type: ${data.loader.train.prof_type}
      coordinates: ${data.loader.train.coordinates}
      targets: ${data.loader.train.targets}
      results:
        hofx:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          normalization: ${vars.hofx.normalization}
        prof:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
    pred:
      prof_type: ${data.vars.prof.type}  # Type of profile to use
      coordinates: ${data.loader.train.coordinates}