# @package _global_
# Experiment-specific Hydra structure

# Overriding default structure
defaults:
  - override /data: default
  - override /paths: default
  - override /hydra: default
  - override /model: inverse_pinnverse
  - override /trainer: cpu
  - override /callbacks: default
  - override /logger: default

# All parameters below will be merged with parameters from default configurations set above.
# This allows you to overwrite only specified parameters

# Task name. Note that it determines the output directory path
task_name: "pinnverse_operator"  # Experiment

# Run id (unique identifier for the run)
paths:
  task_dir: "../inverse_outputs"
  run_id: "version_033"
  data_dir: "../../GOES_ML-main/Data"

# Trainer configuration
trainer:
  min_epochs: 1  # To prevent early stopping
  max_epochs: 10  # Number of epochs

# Model parameters
model:
  _target_: inverse.model.model.PPINNvarOperator3
  optimizer:
    weight_decay: 0.0005  # Weight decay for the optimizer
    lr: 0.0001  # Learning rate for the optimizer
  lr_scheduler: null
  #   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  #   _partial_: true
  #   mode: min
  #   factor: 0.2  # factor_lr_reduce
  #   patience: 3  # patience_lr
  #   min_lr: 1.0e-6  # learning_rate_min
  #   threshold: 0.01  # mindelta_lr_reduce
  loss_func:
    _target_: inverse.model.loss.ForwardVar3  # Load loss function
    _partial_: false  # Partial instantiation for callable
    b_inv:
      _target_: numpy.load
      _args_:
        - ${data.sets.background.covariance.path}
    r_inv:
      _target_: numpy.load
      _args_:
        - ${data.sets.observations.covariance.path}
    lambda_b: 1  # Weight for background error
    lambda_o: 0  # Weight for observation error
  activation:
    _target_: forward.model.activation.Sine
    _partial_: true
  # Positional encoding
  positional_encoding:
    _target_: inverse.model.encoding.GaussianPositionalEncoding  # inverse.model.encoding.IdentityPositionalEncoding
    _partial_: true
    num_freqs: 5
  # Parameters
  parameters:
    data:
      n_levels: 1  # Number of output levels
      n_pressure: 1  # Number of pressure levels
    architecture:
      n_neurons: 64  # Number of neurons in the hidden layers
      n_layers: 4  # Number of hidden layers
      dropout: 0.5  # Dropout rate

# Data location
data:
  _target_: inverse.model.data_loader.PINNverseDataloader
  inputs: ['lat', 'lon', 'scans', 'surf', 'meta', 'pressure'] # Model inputs
  outputs: ['hofx', 'prof', 'background', 'background_err']  # Model outputs
  batch_size: 32  # Batch size
  num_workers: 1  # Number of workers for data loading
  sets:
    background:
      covariance:
        path: ${data.sets.train.path}/b_covariance.npy
    observations:
      covariance:
        path: ${data.sets.train.path}/r_covariance.npy
    train:
      path: ${data.dir}/Test2
      split:
        start: 0
        end: 256608
      scans:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
              min: 9.0
              max: 180.0
              mean: 88.57895
              stdev: 54.53759
      lat:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
              min: -90.0
              max: 90.0
      lon:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
              min: -180.0
              max: 180.0
      pressure:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
              min: 0.012935171
              max: 928.14125
              mean: 386.31695
              stdev: 344.8104
      prof:
        normalization:
          _target_: inverse.data.transformations.identity
          _partial_: true
      # hofx only contains half of the outputs, so I'm loading my complete outputs
      hofx:
        load:
          _target_: numpy.load
          _args_:
            - ${data.sets.train.path}/hofx_lightning.npy
        normalization:
          _target_: inverse.data.transformations.identity
          _partial_: true
    valid:
      split:
        start: 256608
        end: 341240
      lat:
        normalization: ${data.sets.train.lat.normalization}
      lon:
        normalization: ${data.sets.train.lon.normalization}
      scans:
        normalization: ${data.sets.train.scans.normalization}
      pressure:
        normalization: ${data.sets.train.pressure.normalization}
      path: ${data.dir}/Test2
      # hofx only contains half of the outputs, so I'm loading my complete outputs
      hofx:
        load:
          _target_: numpy.load
          _args_:
            - ${data.sets.valid.path}/hofx_lightning.npy
        normalization: ${data.sets.train.hofx.normalization}
    test:
      path: ${data.dir}/Test2
      split:
          start: 341240
          end: 426872
      lat:
        normalization: ${data.sets.train.lat.normalization}
      lon:
        normalization: ${data.sets.train.lon.normalization}
      scans:
        normalization: ${data.sets.train.scans.normalization}
      pressure:
        normalization: ${data.sets.train.pressure.normalization}
      # hofx only contains half of the outputs, so I'm loading my complete outputs
      hofx:
        load:
          _target_: numpy.load
          _args_:
            - ${data.sets.test.path}/hofx_lightning.npy
        normalization: ${data.sets.train.hofx.normalization}
      results:
        hofx:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          normalization: ${data.sets.train.hofx.normalization}
        prof:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy

# Logger
logger:
  offline: True  # Save logs to file
  ui: True  # Open the logger UIs
