# @package _global_
# Experiment-specific Hydra structure

# Overriding default structure
defaults:
  - override /data: default
  - override /paths: default
  - override /hydra: default
  - override /model: inverse_pinnverse
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: default

# All parameters below will be merged with parameters from default configurations set above.
# This allows you to overwrite only specified parameters

# Task name. Note that it determines the output directory path
task_name: "pinnverse_operator"  # Experiment

# Run id (unique identifier for the run)
paths:
  task_dir: "../inverse_outputs"
  run_id: "version_103"
  data_dir: "../../GOES_ML-main/Data"

# Trainer configuration
trainer:
  min_epochs: 1  # To prevent early stopping
  max_epochs: 32  # Number of epochs

# Model parameters
model:
  _target_: inverse.model.model.PINNverseOperatorMulti
  optimizer:
    lr: 0.001  # Learning rate for the optimizer
    weight_decay: 0.0005  # Weight decay for the optimizer
  lr_scheduler: null  # No learning rate scheduler
  loss_func:
    _target_: inverse.model.loss.ForwardVar3 # Load loss function
    _partial_: false  # Partial instantiation for callable
    b_inv:
      _target_: numpy.load
      _args_:
        - ${data.sets.background.covariance.path}
    r_inv:
      _target_: numpy.load
      _args_:
        - ${data.sets.observations.covariance.path}
    lambda_b: 1  # Weight for background error
    lambda_o: 1  # Weight for observation error
  activation_in:
    _target_: forward.model.activation.Sine
    _partial_: true
  positional_encoding:
    _target_: inverse.model.encoding.IdentityPositionalEncoding
    _partial_: true
    # num_freqs: 20
  parameters:
    data:
      n_levels: 1  # Number of output levels
      n_pressure: 1  # Number of pressure levels
    architecture:
      n_neurons: 64  # Number of neurons in the hidden layers
      n_layers: 4  # Number of hidden layers
      dropout: 0.1  # Dropout rate
  # Activation function (output layer)
  activation_out:
    _target_: torch.nn.Sigmoid
    _partial_: true
  # Transformations (output layer)
  transform:
    _target_: inverse.data.transformations.min_max
    _partial_: true
    stats:
      _target_: inverse.data.statistics.read_statistics_var
      _args_:
        - ${model.parameters.data.statistics.path}
        - 'prof'
    inverse_transform: false
  # Transformations (loss function)
  inverse_transform:
    _target_: inverse.data.transformations.min_max
    _partial_: true
    stats:
      _target_: inverse.data.statistics.read_statistics_var
      _args_:
        - ${model.parameters.data.statistics.path}
        - 'prof'
    inverse_transform: true
  #clip:
  #  _target_: inverse.data.transformations.clip
  #  _partial_: true
  #  stats:
  #    _target_: inverse.data.statistics.read_statistics_var
  #    _args_:
  #      - ${model.parameters.data.statistics.path}
  #      - 'prof'

# Data location
data:
  _target_: inverse.model.data_loader.PINNverseDataloader
  clrsky_filter: True  # Use clear sky data
  inputs: [ 'lat', 'lon', 'scans', 'surf', 'meta', 'pressure' ] # Model inputs
  outputs: [ 'hofx', 'prof', 'background', 'background_err' ]  # Model outputs
  # pin_memory: False  # Pin memory for faster data transfer to GPU
  num_workers: 1  # Number of workers for data loading
  batch_size: 32  # Batch size
  sets:
    background:
      covariance:
        path: ${data.sets.train.path}/b_covariance_cloudy_norm.npy
    observations:
      covariance:
        path: ${data.sets.train.path}/r_covariance_cloudy_norm.npy
    train:
      path: ${data.dir}/Test2
      split:
        start: 0
        end: 256608
      scans:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
            min: 9.0
            max: 180.0
            mean: 88.57895
            stdev: 54.53759
      lat:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
            min: -90.0
            max: 90.0
      lon:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
            min: -180.0
            max: 180.0
      pressure:
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
            min: 0.012935171
            max: 928.14125
            mean: 386.31695
            stdev: 344.8104
      prof:
        normalization: ${model.transform}
      # hofx only contains half of the outputs, so I'm loading my complete outputs
      hofx:
        load:
          _target_: numpy.load
          _args_:
            - ${data.sets.train.path}/hofx_lightning.npy
        normalization:
          _target_: inverse.data.transformations.identity
          _partial_: true
      clrsky:
        save:
            _target_: numpy.save
            _partial_: true
            _args_:
                - ${data.sets.train.path}/clrsky.npy
        load:
            _target_: numpy.load
            _args_:
                - ${data.sets.train.path}/clrsky.npy
        normalization:
          _target_: inverse.data.transformations.min_max
          _partial_: true
          stats:
            min: 0.0
            max: 1.0
            mean: 0.5
            stdev: 0.28867513
    valid:
      path: ${data.dir}/Test2
      split:
        start: 256608
        end: 341240
      lat:
        normalization: ${data.sets.train.lat.normalization}
      lon:
        normalization: ${data.sets.train.lon.normalization}
      scans:
        normalization: ${data.sets.train.scans.normalization}
      pressure:
        normalization: ${data.sets.train.pressure.normalization}
      # hofx only contains half of the outputs, so I'm loading my complete outputs
      hofx:
        load:
          _target_: numpy.load
          _args_:
            - ${data.sets.valid.path}/hofx_lightning.npy
        normalization: ${data.sets.train.hofx.normalization}
      prof:
        normalization: ${data.sets.train.prof.normalization}
    test:
      path: ${data.dir}/Test2
      split:
        start: 341240
        end: 426872
      lat:
        normalization: ${data.sets.train.lat.normalization}
      lon:
        normalization: ${data.sets.train.lon.normalization}
      scans:
        normalization: ${data.sets.train.scans.normalization}
      pressure:
        normalization: ${data.sets.train.pressure.normalization}
      # hofx only contains half of the outputs, so I'm loading my complete outputs
      hofx:
        load:
          _target_: numpy.load
          _args_:
            - ${data.sets.test.path}/hofx_lightning.npy
        normalization: ${data.sets.train.hofx.normalization}
      results:
        hofx:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_hofx.npy
          normalization: ${data.sets.train.hofx.normalization}
        prof:
          save:
            _target_: numpy.save
            _partial_: true
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy
          load:
            _target_: numpy.load
            _args_:
              - ${paths.checkpoint_dir}/${task_name}_test_prof.npy

# Logger
logger:
  offline: True  # Save logs to file
  ui: True  # Open the logger UIs
