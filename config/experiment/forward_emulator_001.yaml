# @package _global_
# Experiment-specific Hydra structure

# Overriding default structure
defaults:
  - override /paths: default
  - override /hydra: default
  - override /data: forward_default_001
  - override /preparation: forward_default
  - override /loader: forward_default
  - override /model: forward_emulator
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: default

# All parameters below will be merged with parameters from default configurations set above.
# This allows you to overwrite only specified parameters

# Task name. Note that it determines the output directory path
task_name: "forward_emulator"  # Experiment

# Run id (unique identifier for the run)
paths:
  task_dir: "../forward_outputs"
  run_id: "version_001"
  data_dir: "../../GOES_ML-main/Data"
  # checkpoint_dir: "forward/model/checkpoints"

# Trainer configuration
trainer:
  min_epochs: 1  # To prevent early stopping
  max_epochs: 32  # Number of epochs

# Data configuration
data:
  # Precision
  dtype: 'float32'
  # Split ratios for training, validation, and test sets
  stage:
    # Training set
    train:
      # Variables
      vars:
        # Atmospheric profiles
        prof:
          # Normalization function and parameters
          normalization:
            _target_: inverse.data.transformations.min_max
            _partial_: true
            stats:
              _target_: inverse.data.statistics.read_statistics_var
              path: ${preparation.statistics.data.output.path}
              var: 'prof'
              dtype: ${data.dtype}
            axis: 1
    # Validation set
    valid:
      # Variables
      vars:
        # Atmospheric profiles
        prof:
          # Normalization function and parameters
          normalization: ${data.stage.train.vars.prof.normalization}
    # Test set
    test:
      # Variables
      vars:
        # Atmospheric profiles
        prof:
          # Normalization function and parameters
          normalization: ${data.stage.train.vars.prof.normalization}

# Preparation configuration
preparation:
  # Statistics of the dataset
  statistics:
    # Statistics of the training set
    data:
      # Function
      _target_: inverse.data.statistics.compute_statistics
      _recursive_: false  # Do not instantiate nested objects
      # What the method needs as input
      input:
        prof:
          load:
            _target_: utilities.io.load_stack
            _recursive_: false
            stack:
              - ${data.stage.train.vars.prof}
              - ${data.stage.valid.vars.prof}
              - ${data.stage.test.vars.prof}
        surf:
          load:
            _target_: utilities.io.load_stack
            _recursive_: false
            stack:
              - ${data.stage.train.vars.surf}
              - ${data.stage.valid.vars.surf}
              - ${data.stage.test.vars.surf}
        meta:
          load:
            _target_: utilities.io.load_stack
            _recursive_: false
            stack:
              - ${data.stage.train.vars.meta}
              - ${data.stage.valid.vars.meta}
              - ${data.stage.test.vars.meta}
        hofx:
          load:
            _target_: utilities.io.load_stack
            _recursive_: false
            stack:
              - ${data.stage.train.vars.hofx}
              - ${data.stage.valid.vars.hofx}
              - ${data.stage.test.vars.hofx}
      # What the method produces as output (pickle file)
      output:
        path: ${paths.data_dir}/normalization/statistics_cloud_filter.pkl

# Loader configuration
loader:
  stage:
    # Training set
    train:
      # Input
      input:
        cloud_filter:
          load: ${data.stage.train.vars.cloud_filter.load}
    # Validation set
    valid:
      # Input
        input:
          cloud_filter:
            load: ${data.stage.valid.vars.cloud_filter.load}

# Logger configuration
logger:
  # MLflow logger
  ports:
    mlflow: 5001

# Callbacks configuration
callbacks:
  # Figure logger callback
  figure_logger:
    _target_: forward.model.callback.FigureLogger