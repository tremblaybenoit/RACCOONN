# @package _global_
# Experiment-specific Hydra structure

# Overriding default structure
defaults:
  - override /data: inverse
  - override /paths: default
  - override /hydra: default
  - override /model: inverse_pinnverse
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: default

# All parameters below will be merged with parameters from default configurations set above.
# This allows you to overwrite only specified parameters

# Task name. Note that it determines the output directory path
task_name: "pinnverse_operator"  # Experiment

# Run id (unique identifier for the run)
paths:
  task_dir: "../inverse_outputs"
  run_id: "version_200"
  data_dir: "../../GOES_ML-main/Data"

# Trainer configuration
trainer:
  min_epochs: 1  # To prevent early stopping
  max_epochs: 32  # Number of epochs

# Model parameters
model:
  _target_: inverse.model.model.PINNverseOperatorMulti2
  optimizer:
    lr: 0.001  # Learning rate for the optimizer
    weight_decay: 0.000  # Weight decay for the optimizer
  lr_scheduler: null  # No learning rate scheduler
  loss_func:
    _target_: inverse.model.loss.VarLoss # Load loss function
    _partial_: false  # Partial instantiation for callable
    lambda_obs: 0.  # Weight for observation error in the observation space
    lambda_model: 1.  # Weight for background error
    lambda_bcs: 1.  # Weight for boundary conditions error
    loss_obs:
      _target_: inverse.model.loss.diagonal_quadratic_form
      _partial_: true  # function
    loss_model:
      _target_: inverse.model.loss.mse  # inverse.model.loss.diagonal_quadratic_form
      _partial_: true  # function
    loss_bcs:
      _target_: inverse.model.loss.mse  # inverse.model.loss.diagonal_quadratic_form
      _partial_: true  # function
    forward_prof_norm:
      _target_: forward.data.transformations.NormalizeProfiles
      _partial_: false
      profmax:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/prof_div.txt
      profmin:
        _target_: numpy.loadtxt
        _args_:
          - forward/data/normalization/prof_m.txt
    sanity_check: false  # Sanity check for the loss function
    # prof_mask: true  # Apply mask to the profiles  # TODO: Fix this
  activation_in:
    _target_: forward.model.activation.Sine
    _partial_: true
  positional_encoding:
    _target_: inverse.model.encoding.IdentityPositionalEncoding
    _partial_: true
    # num_freqs: 20
  parameters:
    data:
      n_levels: 1  # Number of output levels
      n_pressure: 1  # Number of pressure levels
    architecture:
      n_neurons: 64  # Number of neurons in the hidden layers
      n_layers: 4  # Number of hidden layers
      dropout: 0.1  # Dropout rate
  # Activation function (output layer)
  activation_out:
    _target_: torch.nn.Sigmoid
    _partial_: true
  # Transformations (output layer)
  transform:
    _target_: inverse.data.transformations.min_max
    _partial_: true
    stats:
      _target_: inverse.data.statistics.read_statistics_var
      _args_:
        - ${model.parameters.data.statistics.path}
        - 'prof'
    inverse_transform: false
    # axis: 1
  # Transformations (loss function)
  inverse_transform:
    _target_: inverse.data.transformations.min_max
    _partial_: true
    stats:
      _target_: inverse.data.statistics.read_statistics_var
      _args_:
        - ${model.parameters.data.statistics.path}
        - 'prof'
    inverse_transform: true
    # axis: 1
  #clip:
  #  _target_: inverse.data.transformations.clip
  #  _partial_: true
  #  stats:
  #    _target_: inverse.data.statistics.read_statistics_var
  #    _args_:
  #      - ${model.parameters.data.statistics.path}
  #      - 'prof'


# Logger
logger:
  offline: True  # Save logs to file
  ui: True  # Open the logger UIs