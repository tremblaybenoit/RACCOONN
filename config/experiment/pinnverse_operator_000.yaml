# @package _global_
# Experiment-specific Hydra structure

# Overriding default structure
defaults:
  - override /data: inverse
  - override /preparation: default
  - override /loader: inverse_train  # Training only
  - override /paths: default
  - override /hydra: default
  - override /model: pinnverse_operator
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: default

# All parameters below will be merged with parameters from default configurations set above.
# This allows you to overwrite only specified parameters

# Task name. Note that it determines the output directory path
task_name: "pinnverse_operator"  # Experiment

# Run id (unique identifier for the run)
paths:
  task_dir: "../inverse_outputs"
  run_id: "version_000"
  data_dir: "../../GOES_ML-main/Data"

# Trainer configuration
trainer:
  min_epochs: 1  # To prevent early stopping
  max_epochs: 32  # Number of epochs

# Data configuration
loader:
  stage:
    train:
      obs:
        prof_norm_background: ${data.vars.prof}
        prof_background:
          load: ${data.vars.prof.load}

# Model parameters
model:
  # Model
  _target_: inverse.model.model.PINNverseOperator

  # Optimizer function and parameters
  optimizer:
    # Function
    _target_: torch.optim.Adam
    _partial_: true
    # Learning rate
    lr: 0.001
    # Weight decay (L2 regularization)
    weight_decay: 0.000

  # Learning rate scheduler
  lr_scheduler:
    # Function
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    # Mode for the scheduler
    mode: min
    # Factor to reduce the learning rate
    factor: 0.2
    # Number of epochs with no improvement after which learning rate will be reduced
    patience: 3
    # Minimum learning rate
    min_lr: 1.0e-6
    # Threshold for measuring the new optimum, to only focus on significant changes
    threshold: 0.01

  # Model parameters associated with the data (e.g., input/output dimensions) and architecture
  parameters:
    # Data parameters
    data:
      # Number of variables
      n_lat: 1
      n_lon: 1
      n_scans: 1
      n_prof: 9
      n_levels: 1
      n_pressure: 1
      n_cloud: 0
      # Profile variables (input/output)
      prof_vars: ${data.vars.prof.type}
    # Model architecture
    architecture:
      # Number of neurons in the hidden layers
      n_neurons: 64
      # Number of hidden layers
      n_layers: 4
      # Dropout rate
      dropout: 0.1
  # Loss function and parameters
  loss_func:
    # Function
    _target_: inverse.model.loss.VarLoss # Load loss function
    _partial_: false  # Partial instantiation for callable
    # Forward model to compute the model equivalent of the observations
    forward_model:
      # Function
      _target_: inverse.model.loss.ForwardModel
      # Normalization for the profile variables
      prof_norm:
        # Function
        _target_: forward.data.transformations.NormalizeProfiles
        # Min/max values for normalization
        profmax:
          _target_: numpy.loadtxt
          _args_:
            - forward/data/normalization/prof_div.txt
        profmin:
          _target_: numpy.loadtxt
          _args_:
            - forward/data/normalization/prof_m.txt
    # Weights for observation (obs), model, and boundary condition (bcs) errors
    lambda_obs: 1.
    lambda_model: 1.
    lambda_bcs: 1.
    # Loss term for observations
    loss_obs:
      _target_: inverse.model.loss.mse
    # Loss term for model variables
    loss_model:
      _target_: inverse.model.loss.mse
    # Loss term for boundary conditions
    loss_bcs:
      _target_: inverse.model.loss.mse
    # Pressure filter: Consider only pressure levels with non-zero variance (for model and bcs errors)
    pressure_filter:  ${data.vars.pressure_filter.load}

  # Positional encoding
  positional_encoding:
    _target_: inverse.model.encoding.IdentityPositionalEncoding
    _partial_: true

  # Activation functions (input layer/within the network)
  activation_in:
    _target_: forward.model.activation.Sine
    _partial_: true

  # Activation function (output layer)
  activation_out:
    _target_: torch.nn.Sigmoid
    _partial_: true

  # Transformations (to apply after the output layer)
  transform_out:
    # Inverse normalization for profile variables
    - _target_: ${data.vars.prof.normalization._target_}
      _partial_: true
      stats: ${data.vars.prof.normalization.stats}
      axis: ${data.vars.prof.normalization.axis}  # Axis for normalization
      inverse_transform: true

  # Plot logger
  log_valid: true